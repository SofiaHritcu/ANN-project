{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "def transformImage(filename):\n",
    "    image = Image.open(filename)\n",
    "    # convert image to numpy array\n",
    "    data = asarray(image)\n",
    "    return data\n",
    "\n",
    "inputs = []\n",
    "for img in range(18,25):\n",
    "    filename = str(img)+'.png'\n",
    "    data = transformImage(filename)\n",
    "    pixels = []\n",
    "    means = []\n",
    "    for i in range(0,len(data)):\n",
    "        localMean = 0.0\n",
    "        for j in range(0,len(data[i])):\n",
    "            mean = 0.0\n",
    "            for d in range(0,4):\n",
    "                mean += data[i][j][d]\n",
    "            mean /= 4\n",
    "            localMean += mean\n",
    "        localMean /= 128\n",
    "        means.append(localMean)\n",
    "    inputs.append(means) \n",
    "for img in range(1,17):\n",
    "    filename = str(img)+'.png'\n",
    "    data = transformImage(filename)\n",
    "    pixels = []\n",
    "    means = []\n",
    "    for i in range(0,len(data)):\n",
    "        localMean = 0.0\n",
    "        for j in range(0,len(data[i])):\n",
    "            mean = 0.0\n",
    "            for d in range(0,4):\n",
    "                mean += data[i][j][d]\n",
    "            mean /= 4\n",
    "            localMean += mean\n",
    "        localMean /= 128\n",
    "        means.append(localMean)\n",
    "    inputs.append(means)\n",
    "    \n",
    "outputs=[]\n",
    "outputNames = ['0','1']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### plot data on classes "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "labels = set(outputs)\n",
    "labelsNames = []\n",
    "noData = len(inputs)\n",
    "print(noData)\n",
    "\n",
    "inputs = inputs[1:]\n",
    "for label in labels:\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for dt in range(len(inputs)):\n",
    "        print(inputs[dt])\n",
    "        if outputs[dt] == label:\n",
    "            x.append((inputs[dt][0]+inputs[dt][1])/2)\n",
    "            y.append((inputs[dt][2]+inputs[dt][3])/2)\n",
    "    plt.scatter(x, y, label = label)\n",
    "\n",
    "plt.xlabel('mean sepal')\n",
    "plt.ylabel('mean petal')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### separating training data from test data 20%-80%"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import math\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "# generate the positions of the data that will be a part of the test data\n",
    "noTestDataIndexes = math.floor( 0.2 * len(inputs))\n",
    "testDataIndexes = []\n",
    "for index in range(0,noTestDataIndexes):\n",
    "    testDataIndexes.append(np.random.randint(0,len(inputs)))\n",
    "\n",
    "inputTest = []\n",
    "outputTest = []\n",
    "inputTraining = []\n",
    "outputTraining = []\n",
    "for i in range (0,len(inputs)):\n",
    "    if i in testDataIndexes :\n",
    "        inputTest.append(inputs[i])\n",
    "        outputTest.append(outputs[i])\n",
    "    else:\n",
    "        inputTraining.append(inputs[i])\n",
    "        outputTraining.append(outputs[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**normalize data**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def normalisation(trainData, testData):\n",
    "    scaler = StandardScaler()\n",
    "    if not isinstance(trainData[0], list):\n",
    "        #encode each sample into a list\n",
    "        trainData = [[d] for d in trainData]\n",
    "        testData = [[d] for d in testData]\n",
    "        \n",
    "        scaler.fit(trainData)  #  fit only on training data\n",
    "        normalisedTrainData = scaler.transform(trainData) # apply same transformation to train data\n",
    "        normalisedTestData = scaler.transform(testData)  # apply same transformation to test data\n",
    "        \n",
    "        #decode from list to raw values\n",
    "        normalisedTrainData = [el[0] for el in normalisedTrainData]\n",
    "        normalisedTestData = [el[0] for el in normalisedTestData]\n",
    "    else:\n",
    "        scaler.fit(trainData)  #  fit only on training data\n",
    "        normalisedTrainData = scaler.transform(trainData) # apply same transformation to train data\n",
    "        normalisedTestData = scaler.transform(testData)  # apply same transformation to test data\n",
    "    return normalisedTrainData, normalisedTestData\n",
    "\n",
    "\n",
    "normalizedTrainInput, normalizedTestInput = normalisation(inputTraining,inputTest)\n",
    "print(normalizedTrainInput)\n",
    "print(normalizedTestInput)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.hist(outputTraining, rwidth = 0.8)\n",
    "plt.xticks(np.arange(len(outputNames)), outputNames)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    **ANN Classification (logic)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self,w=[],out=None,delta=0.0):\n",
    "        self.weights = w\n",
    "        self.output = out\n",
    "        self.delta = delta\n",
    "    \n",
    "def initialisation(inputNumberNeurons,outputNumberNeurons,hiddenNeuronsNumber):\n",
    "    networkStruct = []\n",
    "    hiddenLayer = []\n",
    "    # construct the hidden layer  \n",
    "    for neuron in range(0,hiddenNeuronsNumber):\n",
    "        # each neuron starts with random weights \n",
    "        neuronWeights = []\n",
    "        for weight in range(0,inputNumberNeurons+1):\n",
    "            neuronWeights.append(random())\n",
    "        neuronToAdd = Neuron(neuronWeights)\n",
    "        # add each neuron to the hidden layer\n",
    "        hiddenLayer.append(neuronToAdd)\n",
    "    # add the hidden layer to the structure of the network\n",
    "    networkStruct.append(hiddenLayer)\n",
    "    \n",
    "    outputLayer = []        \n",
    "    # construct the output layer\n",
    "    for neuron in range(0,outputNumberNeurons):\n",
    "        # each neuron starts with random weights \n",
    "        neuronWeights = []\n",
    "        for weight in range(0,hiddenNeuronsNumber+1):\n",
    "            neuronWeights.append(random())\n",
    "        neuronToAdd =  Neuron(neuronWeights)\n",
    "        # add each neuron to the output layer\n",
    "        outputLayer.append(neuronToAdd)\n",
    "    # add the output layer to the structure of the network\n",
    "    networkStruct.append(outputLayer)\n",
    "    return networkStruct\n",
    "\n",
    "        \n",
    "def activateNeuron(inputValues,weights):\n",
    "    res = 0.0\n",
    "    for i in range(0,len(inputValues)):\n",
    "        res += inputValues[i] * weights[i]\n",
    "    # adding w0\n",
    "    res += weights[len(inputValues)]\n",
    "    return res\n",
    "\n",
    "def activationFunction(info):\n",
    "    return 1/(1+math.exp(-info))\n",
    "\n",
    "def forwardPropagationInformation(network , inputValues):\n",
    "    # each layer compute the information through the activation of every neuron of the layer\n",
    "    for layer in network:\n",
    "        computedInputs = []\n",
    "        for neuron in layer : \n",
    "            # gather the information for one neuron and compute the information based on the weights\n",
    "            getInformationForNeuron = activateNeuron(inputValues,neuron.weights)\n",
    "            # apply activation function\n",
    "            neuron.output  = activationFunction(getInformationForNeuron)\n",
    "            computedInputs.append(neuron.output)\n",
    "        # transfer the outputs from the k-layer to the (k+1) layer as input\n",
    "        inputValues = computedInputs\n",
    "    # the computedInputs of the last layer is the output\n",
    "    return inputValues\n",
    "\n",
    "\n",
    "def derivativeSigmoid(outputValue):\n",
    "    return  outputValue * ( 1 - outputValue)\n",
    "\n",
    "def backwardPropagationError(network , expectedValues):\n",
    "    # the error is propagated from the last layer of the netwrok to the first\n",
    "    for i in range( len(network)-1,-1,-1):\n",
    "        currentLayer = network[i]\n",
    "        errors = []\n",
    "        # calculate the errors from the output layer\n",
    "        # for each neuron as the error that propagates back\n",
    "        # has to be proportional with the information that came into the neuron\n",
    "        if i == len(network) - 1:\n",
    "            # this is the output layer\n",
    "            for j in range(0,len(currentLayer)):\n",
    "                # iterate through the neurons of the layer\n",
    "                neuron =  currentLayer[j]\n",
    "                # the error is calculated as the difference between the expected value and \n",
    "                # the value of the output layer\n",
    "                errors.append( expectedValues[j] - neuron.output)\n",
    "        else:\n",
    "            # this is an internal layer\n",
    "            # for each neuron we calculate the error gathered from \n",
    "            # the neuron fron the (k+1) layer\n",
    "            for j in range(0,len(currentLayer)):\n",
    "                errorValue = 0.0\n",
    "                nextLayer = network[i+1]\n",
    "                for neuron in nextLayer:\n",
    "                    # get the value of the error for each of the next layers' neurons\n",
    "                    errorValue += neuron.weights[j] * neuron.delta\n",
    "                errors.append(errorValue)\n",
    "        #update delta for each neuron of the current layer     \n",
    "        for j in range(0,len(currentLayer)):\n",
    "            currentLayer[j].delta = errors[j] * derivativeSigmoid(currentLayer[j].output)\n",
    "            \n",
    "\n",
    "def updateWeights(network,sample,learningRate):\n",
    "    # update weights for each neuron of the network\n",
    "    for i in range(0,len(network)):\n",
    "        inputValues = sample[:-1]\n",
    "        if i > 0:\n",
    "            # for the layers of the network \n",
    "            # except the input layer the values are the computed ones from the neurons\n",
    "            inputValues = []\n",
    "            for neuron in network[i]:\n",
    "                inputValues.append(neuron.output)\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputValues)):\n",
    "                neuron.weights[j] += learningRate * neuron.delta * inputValues[j]\n",
    "            neuron.weights[-1] += learningRate * neuron.delta \n",
    "\n",
    "MAX = 10\n",
    "def sparseCategorialCrossentropy(realLabels,computedOutputs):\n",
    "        labelNames = list(set(realLabels))\n",
    "        labelNames.sort()\n",
    "        realClasses=[]\n",
    "        for c in realLabels:\n",
    "            realClasses.append(labelNames.index(c)+1)\n",
    "        predictedClasses=[]\n",
    "        for p in computedOutputs:\n",
    "            max = p[0]\n",
    "            index = 1 \n",
    "            if p[1] > max:\n",
    "                index = 2\n",
    "                max = p[1]\n",
    "            if p[2] > max:\n",
    "                index = 3\n",
    "            predictedClasses.append(index)\n",
    "        sparseCatCrossentropy=0\n",
    "        nr=0\n",
    "        maxVal = -9999999\n",
    "    \n",
    "        for i in range(0,len(realClasses)):\n",
    "                if predictedClasses[i] == 1:\n",
    "                    nr += 1\n",
    "                    sparseCatCrossentropy += MAX\n",
    "                else:\n",
    "                    val = realClasses[i]*math.log(predictedClasses[i]) + (1-realClasses[i])*math.log(abs(1-predictedClasses[i]))\n",
    "                    sparseCatCrossentropy += val\n",
    "                    if val > maxVal:\n",
    "                        maxVal = val\n",
    "        sparseCatCrossentropy = sparseCatCrossentropy - MAX*nr + maxVal\n",
    "        return -1/len(realClasses)*sparseCatCrossentropy\n",
    "        \n",
    "def training(network,data,outputs,numberClasses,learningRate,noEpochs):\n",
    "    \n",
    "    for epoch in range(0,noEpochs):\n",
    "        print('Epoch '+str(epoch)+'loss=')\n",
    "        overAllError = 0.0\n",
    "        nrOut = 0 \n",
    "        lossComputedValues = []\n",
    "        for sample in data :\n",
    "            inputValues = sample\n",
    "            computedValues = forwardPropagationInformation(network,inputValues)\n",
    "            lossComputedValues.append(computedValues)\n",
    "            expected = [ 0 for _ in range(numberClasses)]\n",
    "            expected[outputs[nrOut]] = 1\n",
    "            computedLabels = [0 for _ in range(numberClasses)]\n",
    "            computedLabels[computedValues.index(max(computedValues))] = 1\n",
    "            computedValues = computedLabels\n",
    "            currentError = sum([(expected[i] - computedValues[i]) ** 2 for i in range(0, len(expected))])\n",
    "            overAllError += currentError\n",
    "            backwardPropagationError(network,expected)\n",
    "            updateWeights(network,sample,learningRate)\n",
    "            nrOut += 1\n",
    "        print(sparseCategorialCrossentropy(outputs,lossComputedValues))\n",
    "        \n",
    "def eval(network,data,numberClasses):\n",
    "    computedOutputs = []\n",
    "    for dt in data:\n",
    "        print(dt)\n",
    "        computedOutput = forwardPropagationInformation(network,dt[:-1])\n",
    "        computedLabels = [0 for i in range(numberClasses)]\n",
    "        computedLabels[computedOutput.index(max(computedOutput))] = 1\n",
    "        computedOutput = computedLabels\n",
    "        computedOutputs.append(computedOutput.index(max(computedOutput)))\n",
    "    return computedOutputs\n",
    "        \n",
    "        \n",
    "def evalMultiClass(realLabels, computedLabels, labelNames):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    confMatrix = confusion_matrix(realLabels, computedLabels)\n",
    "    acc = sum([confMatrix[i][i] for i in range(len(labelNames))]) / len(realLabels)\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    for i in range(len(labelNames)):\n",
    "        precision[labelNames[i]] = confMatrix[i][i] / sum([confMatrix[j][i] for j in range(len(labelNames))])\n",
    "        recall[labelNames[i]] = confMatrix[i][i] / sum([confMatrix[i][j] for j in range(len(labelNames))])\n",
    "    return acc, precision, recall, confMatrix\n",
    "            \n",
    "        \n",
    "            \n",
    "            "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### leran model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "\n",
    "numberClasses = len(set(outputs))\n",
    "numberInputNeurons = len(normalizedTrainInput)\n",
    "network = initialisation(numberInputNeurons,numberClasses,10)\n",
    "training(network,normalizedTrainInput,outputTraining,numberClasses,0.1,100)\n",
    "computedOutputs =  eval(network,normalizedTrainInput,numberClasses)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## predict classes for test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training(network,normalizedTestInput,outputTest,numberClasses,0.1,100)\n",
    "computedOutputs =  eval(network,normalizedTestInput,numberClasses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### calculate error\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "error = 0.0\n",
    "for guess, real in zip(computedOutputs, outputTest):\n",
    "    if guess != real:\n",
    "        error += 1\n",
    "error = error / len(outputTest)\n",
    "print(\"classification error (manual) with code: \", error)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "error = 1 - accuracy_score(outputTest, computedOutputs)\n",
    "print(\"classification error (tool) with code: \", error)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputNames = ['0','1','2','3','4','5','6','7','8','9']\n",
    "acc, precision, recall,matrix = evalMultiClass(np.array(outputTest), computedOutputs, outputNames)\n",
    "print('acc: ', acc)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}